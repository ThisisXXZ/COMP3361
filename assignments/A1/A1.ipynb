{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPQt6RQ5rFij",
        "outputId": "5a17442a-5dc8-48e5-82ef-966680eb5cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.3) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.26.4 nltk==3.9.1 scikit-learn==1.6.1 gensim==4.3.3 setuptools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksx_pV88rFik",
        "outputId": "791957e5-648c-438f-e6e2-da080e267df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-56c000555d47>:14: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ numpy version 1.26.4\n",
            "✓ nltk version 3.9.1\n",
            "✓ scikit-learn version 1.6.1\n",
            "✓ gensim version 4.3.3\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup and Validation\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Required package versions\n",
        "required_packages = {\n",
        "    'numpy': '1.26.4',\n",
        "    'nltk': '3.9.1',\n",
        "    'scikit-learn': '1.6.1',\n",
        "    'gensim': '4.3.3'\n",
        "}\n",
        "\n",
        "def validate_environment():\n",
        "    import pkg_resources\n",
        "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "    for package, min_version in required_packages.items():\n",
        "        if package not in installed:\n",
        "            print(f\"❌ {package} not found!\")\n",
        "            return False\n",
        "        if pkg_resources.parse_version(installed[package]) < pkg_resources.parse_version(min_version):\n",
        "            print(f\"❌ {package} version {installed[package]} is below minimum {min_version}\")\n",
        "            return False\n",
        "        print(f\"✓ {package} version {installed[package]}\")\n",
        "    return True\n",
        "\n",
        "assert validate_environment(), \"Please install required packages with correct versions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YsSAtTqt7Q8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "826b3c3f-d0fa-4fa8-c38b-87e330ce0b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-28 14:11:09--  https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/lm/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8441444 (8.0M) [text/plain]\n",
            "Saving to: ‘data/lm/train.txt’\n",
            "\n",
            "data/lm/train.txt   100%[===================>]   8.05M  45.5MB/s    in 0.2s    \n",
            "\n",
            "2025-02-28 14:11:09 (45.5 MB/s) - ‘data/lm/train.txt’ saved [8441444/8441444]\n",
            "\n",
            "--2025-02-28 14:11:10--  https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/lm/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1671618 (1.6M) [text/plain]\n",
            "Saving to: ‘data/lm/dev.txt’\n",
            "\n",
            "data/lm/dev.txt     100%[===================>]   1.59M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-02-28 14:11:10 (21.3 MB/s) - ‘data/lm/dev.txt’ saved [1671618/1671618]\n",
            "\n",
            "--2025-02-28 14:11:10--  https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/lm/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1669761 (1.6M) [text/plain]\n",
            "Saving to: ‘data/lm/test.txt’\n",
            "\n",
            "data/lm/test.txt    100%[===================>]   1.59M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-02-28 14:11:11 (20.6 MB/s) - ‘data/lm/test.txt’ saved [1669761/1669761]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7UPK7sDrFik"
      },
      "source": [
        "### Instructions:\n",
        "1. Implement a Vocabulary class that:\n",
        "   - Handles OOV words (tokens occurring < 3 times)\n",
        "   - Includes special tokens \\<UNK>, \\<START>, \\<END>\n",
        "   - Provides word2idx and idx2word mappings\n",
        "2. Expected vocabulary size: 24,067 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q-rNT_QL8Dvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3315de-a646-4a2c-f6b1-6ff82ffb8218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 24067\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Building vocabulary\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from typing import List, Dict\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<UNK>': 0, '<START>': 1, '<END>': 2}\n",
        "        self.idx2word = ['<UNK>', '<START>', '<END>']\n",
        "        self.word_counts = {}\n",
        "\n",
        "    def build_vocab(self, filepath: str, min_freq: int = 3) -> None:\n",
        "        \"\"\"Build vocabulary from training file\n",
        "\n",
        "        Args:\n",
        "            filepath: Path to training file\n",
        "            min_freq: Minimum frequency threshold for words\n",
        "        \"\"\"\n",
        "        # TODO: Implement vocabulary building\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            text = file.read().lower()\n",
        "            words = nltk.word_tokenize(text)\n",
        "\n",
        "        self.word_counts = Counter(words)\n",
        "\n",
        "        for word, count in self.word_counts.items():\n",
        "            if count >= min_freq and word not in self.word2idx:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word.append(word)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def get_id(self, word: str) -> int:\n",
        "        \"\"\"Get ID for word (returns UNK if not in vocab)\"\"\"\n",
        "        # TODO: Implement\n",
        "        return self.word2idx.get(word, self.word2idx['<UNK>'])\n",
        "\n",
        "    def get_word(self, idx: int) -> str:\n",
        "        \"\"\"Get word for ID\"\"\"\n",
        "        # TODO: Implement\n",
        "        if idx < len(self.idx2word):\n",
        "            return self.idx2word[idx]\n",
        "        return '<UNK>'\n",
        "\n",
        "# Test vocabulary\n",
        "nltk.download('punkt_tab')\n",
        "vocab = Vocabulary()\n",
        "vocab.build_vocab('data/lm/train.txt')\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "assert len(vocab) == 24067, \"Vocabulary size should be 24,067\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "Discuss the number of parameters in n-gram models here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In n-gram models, the number of parameters grows exponentially with the vocabulary size $|V|$ and the n-gram order $n$, as it equals $|V|^n$.\n",
        "\n",
        "This is because the model must store the probability of each word given its context (the previous $n-1$ words). For example, in a trigram model with the vocabulary we just built, we need a 3D array ``prob[word][context_1][context_2]``. Since each dimension has size $|V|$, the total size of the array is $24,067^3$, which is an extremely large number."
      ],
      "metadata": {
        "id": "Fh0PKcBH6ERj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUSdyJuyrFil"
      },
      "source": [
        "### Instructions:\n",
        "1. Implement an n-gram language model that:\n",
        "   - Counts n-grams from training data\n",
        "   - Computes probabilities\n",
        "   - Calculates perplexity\n",
        "2. Test with both unigram and bigram models\n",
        "3. Report perplexity on train and dev sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ACSfNZGE8Yw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aafed9-21ed-401a-995d-bc0921d3d1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity on train (unigram): 943.86\n",
            "Perplexity on dev (unigram): 879.03\n",
            "Perplexity on train (bigram): 81.47\n",
            "Perplexity on dev (bigram): 3124.34\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "class LanguageModel:\n",
        "    def __init__(self, vocab: Vocabulary, n: int):\n",
        "        \"\"\"Initialize n-gram language model\n",
        "\n",
        "        Args:\n",
        "            vocab: Vocabulary object\n",
        "            n: Order of n-gram model\n",
        "        \"\"\"\n",
        "        self.vocab = vocab\n",
        "        self.n = n\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "\n",
        "    def get_ngrams(self, tokens: List[str], n: int) -> List[Tuple[str]]:\n",
        "        \"\"\"Extract n-grams from token sequence\"\"\"\n",
        "        # TODO: Implement n-gram extraction\n",
        "        ngrams = []\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngrams.append(tuple(tokens[i:i+n]))\n",
        "        return ngrams\n",
        "\n",
        "    def train(self, filepath: str) -> None:\n",
        "        \"\"\"Train n-gram language model\"\"\"\n",
        "        # TODO: Implement training\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                tokens = nltk.word_tokenize(line.lower())\n",
        "                tokens = [self.vocab.idx2word[self.vocab.get_id(token)] for token in tokens]\n",
        "                num_start = self.n - 1 if self.n > 1 else 1\n",
        "                tokens = ['<START>'] * num_start + tokens + ['<END>']\n",
        "\n",
        "                ngrams = self.get_ngrams(tokens, self.n)\n",
        "                contexts = self.get_ngrams(tokens, self.n - 1)\n",
        "\n",
        "                for ngram in ngrams:\n",
        "                    self.ngram_counts[ngram] += 1\n",
        "                for context in contexts:\n",
        "                    self.context_counts[context] += 1\n",
        "\n",
        "\n",
        "    def get_prob(self, word: str, context: Tuple[str]) -> float:\n",
        "        \"\"\"Get probability of word given context\"\"\"\n",
        "        # TODO: Implement probability calculation\n",
        "        ngram = context + (word,)\n",
        "        if self.context_counts[context] == 0:\n",
        "            return 0\n",
        "        return self.ngram_counts[ngram] / self.context_counts[context]\n",
        "\n",
        "    def perplexity(self, filepath: str) -> float:\n",
        "        \"\"\"Calculate perplexity on given text\"\"\"\n",
        "        # TODO: Implement perplexity calculation\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            log_prob_sum = 0\n",
        "            num_words = 0\n",
        "            for line in file:\n",
        "                tokens = nltk.word_tokenize(line.lower())\n",
        "                tokens = [self.vocab.idx2word[self.vocab.get_id(token)] for token in tokens]\n",
        "                num_start = self.n - 1 if self.n > 1 else 1\n",
        "                tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']\n",
        "\n",
        "                for i in range(len(tokens) - self.n + 1):\n",
        "                    ngram = tuple(tokens[i:i+self.n])\n",
        "                    context = tuple(tokens[i:i+self.n-1])\n",
        "                    word = ngram[-1]\n",
        "\n",
        "                    prob = self.get_prob(word, context)\n",
        "                    if prob == 0:\n",
        "                        prob = 1e-10\n",
        "                    log_prob_sum += np.log2(prob)\n",
        "                    num_words += 1\n",
        "\n",
        "        perplexity = 2 ** (-log_prob_sum / num_words)\n",
        "        return perplexity\n",
        "\n",
        "\n",
        "# Test language model\n",
        "lm_unigram = LanguageModel(vocab, n=1)\n",
        "lm_unigram.train('data/lm/train.txt')\n",
        "train_perplexity = lm_unigram.perplexity('data/lm/train.txt')\n",
        "dev_perplexity = lm_unigram.perplexity('data/lm/dev.txt')\n",
        "print(f\"Perplexity on train (unigram): {train_perplexity:.2f}\")\n",
        "print(f\"Perplexity on dev (unigram): {dev_perplexity:.2f}\")\n",
        "\n",
        "lm_bigram = LanguageModel(vocab, n=2)\n",
        "lm_bigram.train('data/lm/train.txt')\n",
        "train_perplexity = lm_bigram.perplexity('data/lm/train.txt')\n",
        "dev_perplexity = lm_bigram.perplexity('data/lm/dev.txt')\n",
        "print(f\"Perplexity on train (bigram): {train_perplexity:.2f}\")\n",
        "print(f\"Perplexity on dev (bigram): {dev_perplexity:.2f}\")\n",
        "\n",
        "assert dev_perplexity > 0, \"Perplexity should be positive\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "Compare unigram and bigram model performance here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unigram and bigram model performance are as follows:\n",
        "\n",
        "|model|perplexity (train set)|perplexity (dev set)|\n",
        "|---|---|---|\n",
        "|unigram|943.86|879.03|\n",
        "|bigram|81.47|3124.34|\n",
        "\n",
        "On the training set, the bigram model achieves a significantly lower perplexity compared to the unigram model. This is because the unigram model does not rely on any contextual information.\n",
        "\n",
        "\n",
        "\n",
        "On the dev set, both models exhibit poor perplexity, with the bigram model performing significantly worse than the unigram model, which is unexpected. This is likely due to the presence of unknown contexts (e.g., unseen bigrams) leads to zero probabilities, and since $\\log_2(0) \\to -\\infty$, this significantly increases perplexity. This effect is more pronounced in the bigram model due to its reliance on context."
      ],
      "metadata": {
        "id": "t6rVY3Gzhnmr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "93_yLu9dVr0C",
        "outputId": "4b4a6715-4d1c-4469-9f55-9722c60453ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add-one perplexity on train: 1291.30\n",
            "Add-one perplexity on dev: 1512.07\n"
          ]
        }
      ],
      "source": [
        "class AddOneLanguageModel(LanguageModel):\n",
        "    def get_prob(self, word: str, context: Tuple[str]) -> float:\n",
        "        \"\"\"Get smoothed probability using add-one smoothing\"\"\"\n",
        "        # TODO: Implement add-one smoothing\n",
        "        ngram = context + (word,)\n",
        "        context_count = self.context_counts[context]\n",
        "        ngram_count = self.ngram_counts[ngram]\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        return (ngram_count + 1) / (context_count + vocab_size)\n",
        "\n",
        "# Test add-one smoothing\n",
        "add_one_lm = AddOneLanguageModel(vocab, n=2)\n",
        "add_one_lm.train('data/lm/train.txt')\n",
        "print(f\"Add-one perplexity on train: {add_one_lm.perplexity('data/lm/train.txt'):.2f}\")\n",
        "print(f\"Add-one perplexity on dev: {add_one_lm.perplexity('data/lm/dev.txt'):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": [
        "Analyze how add-one smoothing affects the model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram model with add-one smoothing performance is as follows:\n",
        "\n",
        "|dataset|perplexity|\n",
        "|---|---|\n",
        "|training set|1291.30|\n",
        "|dev set|1512.07|\n",
        "\n",
        "The bigram model with add-one smoothing significantly reduces perplexity on the dev set but increases perplexity on the training set.\n",
        "\n",
        "On the dev set, previously unseen contexts are now assigned a probability, preventing them from contributing as heavily to perplexity as they did before.\n",
        "\n",
        "However, on the training set, this smoothing dilutes the original probabilities of known contexts, leading to an increase in perplexity. This tradeoff can be viewed as the cost for the bigram model to achieve better generalization."
      ],
      "metadata": {
        "id": "zxTLx0WsXZww"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRlx_9ozrFim"
      },
      "source": [
        "#### Instructions:\n",
        "1. Implement add-k smoothing with configurable k\n",
        "2. Try k values: 0.1, 0.5, 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jhcuJWo7VuNg",
        "outputId": "94a9b6a6-3a01-4b50-9791-3503d48e6817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add-0.01 perplexity on train: 154.54\n",
            "Add-0.01 perplexity on dev: 421.68\n",
            "Add-0.1 perplexity on train: 379.19\n",
            "Add-0.1 perplexity on dev: 648.66\n",
            "Add-0.5 perplexity on train: 878.09\n",
            "Add-0.5 perplexity on dev: 1130.63\n",
            "Add-1.0 perplexity on train: 1291.30\n",
            "Add-1.0 perplexity on dev: 1512.07\n"
          ]
        }
      ],
      "source": [
        "class AddKLanguageModel(LanguageModel):\n",
        "    def __init__(self, vocab: Vocabulary, n: int, k: float):\n",
        "        super().__init__(vocab, n)\n",
        "        self.k = k\n",
        "\n",
        "    def get_prob(self, word: str, context: Tuple[str]) -> float:\n",
        "        \"\"\"Get smoothed probability using add-k smoothing\"\"\"\n",
        "        # TODO: Implement add-k smoothing\n",
        "        ngram = context + (word,)\n",
        "        context_count = self.context_counts[context]\n",
        "        ngram_count = self.ngram_counts[ngram]\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        return (ngram_count + k) / (context_count + k * vocab_size)\n",
        "\n",
        "# Test add-k smoothing with different k values\n",
        "k_values = [0.01, 0.1, 0.5, 1.0]\n",
        "for k in k_values:\n",
        "    add_k_lm = AddKLanguageModel(vocab, n=2, k=k)\n",
        "    add_k_lm.train('data/lm/train.txt')\n",
        "    print(f\"Add-{k} perplexity on train: {add_k_lm.perplexity('data/lm/train.txt'):.2f}\")\n",
        "    print(f\"Add-{k} perplexity on dev: {add_k_lm.perplexity('data/lm/dev.txt'):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": [
        "Compare performance with different k values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram model with add-k smoothing performance is as follows:\n",
        "\n",
        "|dataset|k=0.01|k=0.1|k=0.5|k=1.0 (add-1 smoothing)|\n",
        "|---|---|---|---|---|\n",
        "|training set|186.75|474.68|1132.34|1682.19|\n",
        "|dev set|530.65|829.50|1472.11|1981.10|\n",
        "\n",
        "We can see that as $k$ decreases, the perplexity also decreases.\n",
        "\n",
        "This result shows that add-one smoothing over-smooths and performs worse. Smaller $k$ values better balance fitting training data and generalizing to unseen data, while larger $k$ values dilute known bigram probabilities too aggressively, increasing perplexity on both sets."
      ],
      "metadata": {
        "id": "-NaOIwVHLd5J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GNvAFkErFim"
      },
      "source": [
        "#### Instructions:\n",
        "1. Implement linear interpolation of unigram, bigram, and trigram models\n",
        "2. Initial lambdas: [0.1, 0.3, 0.6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "K4N_XuN6WVCQ",
        "outputId": "a419a2b0-3f9f-4f99-e8e4-0a62bdebc7bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpolated perplexity on train: 13.53\n",
            "Interpolated perplexity on dev: 302.56\n",
            "Interpolated perplexity on test: 307.37\n",
            "\n",
            "Best lambdas found by EM algorithm:  [0.29649361 0.56091062 0.14259577]\n",
            "Interpolated with EM optimization perplexity on train: 27.91\n",
            "Interpolated with EM optimization perplexity on dev: 259.87\n",
            "Interpolated with EM optimization perplexity on test: 263.59\n"
          ]
        }
      ],
      "source": [
        "class InterpolatedLanguageModel:\n",
        "    def __init__(self, vocab: Vocabulary, lambdas: List[float]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab: Vocabulary object\n",
        "            lambdas: List of interpolation weights (should sum to 1)\n",
        "        \"\"\"\n",
        "        assert abs(sum(lambdas) - 1.0) < 1e-6, \"Lambdas must sum to 1\"\n",
        "        self.vocab = vocab\n",
        "        self.lambdas = lambdas\n",
        "        self.max_n = len(self.lambdas)\n",
        "        self.models = [LanguageModel(vocab, n=i+1) for i in range(len(lambdas))]\n",
        "\n",
        "    def train(self, filepath: str) -> None:\n",
        "        \"\"\"Train all n-gram models\"\"\"\n",
        "        for model in self.models:\n",
        "            model.train(filepath)\n",
        "\n",
        "    def EM_optimize(self, filepath: str, max_iter: int = 10, tol: float = 1e-6) -> None:\n",
        "        \"\"\"Optimize hyperparameters on dev dataset\"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            ngrams = []\n",
        "            for line in file:\n",
        "                tokenized_line = nltk.word_tokenize(line.lower())\n",
        "                tokenized_line = [self.vocab.idx2word[self.vocab.get_id(token)] for token in tokenized_line]\n",
        "                num_start = self.max_n - 1 if self.max_n > 1 else 1\n",
        "                tokenized_line = ['<START>'] * num_start + tokenized_line + ['<END>']\n",
        "                ngrams.extend([tuple(tokenized_line[i:i+self.max_n]) for i in range(len(tokenized_line)-self.max_n+1)])\n",
        "\n",
        "        for iteration in range(max_iter):\n",
        "            gamma = np.zeros((len(ngrams), len(self.lambdas)))\n",
        "            for idx, ngram in enumerate(ngrams):\n",
        "                context = ngram[:-1]\n",
        "                word = ngram[-1]\n",
        "                for i, model in enumerate(self.models):\n",
        "                    model_context = context[-(model.n - 1):] if model.n > 1 else ()\n",
        "                    gamma[idx, i] = self.lambdas[i] * model.get_prob(word, model_context)\n",
        "                gamma[idx, :] /= np.sum(gamma[idx, :])\n",
        "\n",
        "            new_lambdas = np.sum(gamma, axis=0) / len(ngrams)\n",
        "\n",
        "            if np.max(np.abs(new_lambdas - self.lambdas)) < tol:\n",
        "                break\n",
        "\n",
        "            self.lambdas = new_lambdas\n",
        "\n",
        "\n",
        "    def get_prob(self, word: str, context: Tuple[str]) -> float:\n",
        "        \"\"\"Get interpolated probability\"\"\"\n",
        "        # TODO: Implement interpolated probability\n",
        "        prob = 0.0\n",
        "        for weight, model in zip(self.lambdas, self.models):\n",
        "            model_context = context[-(model.n - 1):] if model.n > 1 else ()\n",
        "            prob += weight * model.get_prob(word, model_context)\n",
        "        return prob\n",
        "\n",
        "    def perplexity(self, filepath: str) -> float:\n",
        "        \"\"\"Calculate perplexity using interpolated probabilities\"\"\"\n",
        "        # TODO: Implement perplexity calculation\n",
        "\n",
        "        log_prob_sum = 0\n",
        "        num_words = 0\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                tokens = nltk.word_tokenize(line.lower())\n",
        "                tokens = [self.vocab.idx2word[self.vocab.get_id(token)] for token in tokens]\n",
        "                num_start = self.max_n - 1 if self.max_n > 1 else 1\n",
        "                tokens = ['<START>'] * num_start + tokens + ['<END>']\n",
        "\n",
        "                for i in range(len(tokens) - self.max_n + 1):\n",
        "                    ngram = tuple(tokens[i:i+self.max_n])\n",
        "                    context = tuple(tokens[i:i+self.max_n-1])\n",
        "                    word = ngram[-1]\n",
        "\n",
        "                    prob = self.get_prob(word, context)\n",
        "                    if prob == 0:\n",
        "                        prob = 1e-10\n",
        "                    log_prob_sum += np.log2(prob)\n",
        "                    num_words += 1\n",
        "\n",
        "        perplexity = 2 ** (-log_prob_sum / num_words)\n",
        "        return perplexity\n",
        "\n",
        "\n",
        "# Test interpolated model\n",
        "initial_lambdas = [0.2, 0.3, 0.5]  # Example weights for unigram, bigram, trigram\n",
        "interpolated_lm = InterpolatedLanguageModel(vocab, initial_lambdas)\n",
        "interpolated_lm.train('data/lm/train.txt')\n",
        "interpolated_lm_with_em = InterpolatedLanguageModel(vocab, initial_lambdas)\n",
        "interpolated_lm_with_em.train('data/lm/train.txt')\n",
        "interpolated_lm_with_em.EM_optimize('data/lm/dev.txt')\n",
        "\n",
        "\n",
        "print(f\"Interpolated perplexity on train: {interpolated_lm.perplexity('data/lm/train.txt'):.2f}\")\n",
        "print(f\"Interpolated perplexity on dev: {interpolated_lm.perplexity('data/lm/dev.txt'):.2f}\")\n",
        "print(f\"Interpolated perplexity on test: {interpolated_lm.perplexity('data/lm/test.txt'):.2f}\")\n",
        "\n",
        "print(\"\")\n",
        "print(f\"Best lambdas found by EM algorithm: \", interpolated_lm_with_em.lambdas)\n",
        "print(f\"Interpolated with EM optimization perplexity on train: {interpolated_lm_with_em.perplexity('data/lm/train.txt'):.2f}\")\n",
        "print(f\"Interpolated with EM optimization perplexity on dev: {interpolated_lm_with_em.perplexity('data/lm/dev.txt'):.2f}\")\n",
        "print(f\"Interpolated with EM optimization perplexity on test: {interpolated_lm_with_em.perplexity('data/lm/test.txt'):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": [
        "Analyze the effectiveness of interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the initial hyperparameters $\\lambda_1=0.2, \\lambda_2=0.3,\\lambda_3=0.5$, the results are:\n",
        "\n",
        "|training set|dev set|test set|\n",
        "|---|---|---|\n",
        "|13.33|367.98|374.16|\n",
        "\n",
        "\n",
        "This result is better than all smoothing methods we have tried before, which proves the effectivenss of interpolation.\n",
        "\n",
        "The hyperparameters I optimized on dev set are:\n",
        "$$\n",
        "\\lambda_1=0.326, \\lambda_2=0.543, \\lambda_3=0.131\n",
        "$$\n",
        "The corresponding results for this set of parameters are:\n",
        "\n",
        "|training set|dev set|test set|\n",
        "|---|---|---|\n",
        "|30.54|307.84|312.40|\n",
        "\n",
        "This set of hyperparameters further decreases perplexity on dev set, and also achieve better results on test set.\n",
        "\n",
        "The optimization algorithm I used is EM algorithm, and I will discuss about it in the following section (1.3.4 Optimization).\n"
      ],
      "metadata": {
        "id": "_uHXBQm0dfNH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": [
        "I discovered online that the EM algorithm is an efficient method for optimizing the parameters of interpolation in statistical models.\n",
        "\n",
        "The Expectation-Maximization (EM) algorithm is an iterative method for estimating parameters in models with missing or unobserved data. It alternates between computing expected values for the latent variables (E-Step) and updating parameters to maximize the likelihood of the observed data (M-Step), repeating until convergence.\n",
        "\n",
        "I also implemented this algorithm (the `EM_optimize` function) and used it for optimizing the hyperparameters on dev set. The results are quite promising, as shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zlEaIPMrFim"
      },
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SalrJDumrFim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079931ea-8267-46aa-ecfb-e585a4cd5ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W43RouIurFin"
      },
      "source": [
        "## 2.1 Find most similar word\n",
        "### Instructions:\n",
        "1. Implement function to find similar words using cosine similarity\n",
        "2. Expected similarity scores should be between 0 and 1\n",
        "3. Return top 3 similar words for each query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZJb-vHlrFin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e409b66-48f9-4766-ba0e-1540a24a0ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most similar to 'ubiquitous':\n",
            "omnipresent: 0.606\n",
            "commonplace: 0.574\n",
            "everywhere: 0.516\n",
            "\n",
            "Most similar to 'serendipity':\n",
            "happenstance: 0.614\n",
            "serendipitous: 0.450\n",
            "silliness: 0.440\n",
            "\n",
            "Most similar to 'melancholy':\n",
            "melancholic: 0.776\n",
            "wistful: 0.749\n",
            "introspective: 0.691\n",
            "\n",
            "Most similar to 'paradox':\n",
            "paradoxes: 0.640\n",
            "contradiction: 0.503\n",
            "anomaly: 0.494\n",
            "\n",
            "Most similar to 'ethereal':\n",
            "otherworldly: 0.699\n",
            "dreamy: 0.675\n",
            "seductive: 0.613\n",
            "\n",
            "Most similar to 'cacophony':\n",
            "deafening: 0.644\n",
            "cacophonous: 0.606\n",
            "shrill: 0.591\n"
          ]
        }
      ],
      "source": [
        "def find_most_similar(word: str, wv_from_bin) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Find most similar words using cosine similarity\n",
        "\n",
        "    Args:\n",
        "        word: Query word\n",
        "        wv_from_bin: Loaded word vectors\n",
        "\n",
        "    Returns:\n",
        "        List of (word, similarity) tuples\n",
        "    \"\"\"\n",
        "    # TODO: Implement similarity search\n",
        "    try:\n",
        "        similar_words = wv_from_bin.most_similar(word, topn=3)\n",
        "        return similar_words\n",
        "    except KeyError:\n",
        "        return []\n",
        "\n",
        "test_words = [\n",
        "    'ubiquitous',\n",
        "    'serendipity',\n",
        "    'melancholy',\n",
        "    'paradox',\n",
        "    'ethereal',\n",
        "    'cacophony'\n",
        "]\n",
        "for word in test_words:\n",
        "    similar_words = find_most_similar(word, wv_from_bin)\n",
        "    print(f\"\\nMost similar to '{word}':\")\n",
        "    for similar_word, similarity in similar_words:\n",
        "        print(f\"{similar_word}: {similarity:.3f}\")\n",
        "        assert 0 <= similarity <= 1, f\"Invalid similarity score: {similarity}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tFP1H8KrFin"
      },
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L28jEerCrFin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8e00e8-5282-47d7-cd70-ec4969ccae5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analogy: king:queen :: man:?\n",
            "woman: 0.725\n",
            "girl: 0.589\n",
            "she: 0.571\n",
            "\n",
            "Analogy: paris:france :: rome:?\n",
            "italy: 0.773\n",
            "spain: 0.611\n",
            "italian: 0.566\n",
            "\n",
            "Analogy: france:french :: germany:?\n",
            "german: 0.940\n",
            "austrian: 0.699\n",
            "berlin: 0.652\n",
            "\n",
            "Analogy: london:england :: paris:?\n",
            "france: 0.712\n",
            "french: 0.578\n",
            "marseille: 0.555\n",
            "\n",
            "Analogy: cat:kitten :: dog:?\n",
            "puppy: 0.615\n",
            "puppies: 0.499\n",
            "rottweiler: 0.446\n",
            "\n",
            "Analogy: rich:poor :: strong:?\n",
            "weak: 0.655\n",
            "despite: 0.629\n",
            "stronger: 0.573\n"
          ]
        }
      ],
      "source": [
        "def find_analogy(a: str, b: str, c: str, wv_from_bin) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Find word analogies using vector arithmetic\n",
        "\n",
        "    Args:\n",
        "        a, b, c: Words for analogy a:b :: c:?\n",
        "        wv_from_bin: Loaded word vectors\n",
        "\n",
        "    Returns:\n",
        "        List of (word, similarity) tuples\n",
        "    \"\"\"\n",
        "    # TODO: Implement analogy finding\n",
        "    try:\n",
        "        results = wv_from_bin.most_similar(positive=[b, c], negative=[a], topn=3)\n",
        "        return results\n",
        "    except KeyError as e:\n",
        "        return []\n",
        "\n",
        "# Test analogies\n",
        "analogies = [\n",
        "    ('king', 'queen', 'man'),\n",
        "    ('paris', 'france', 'rome'),\n",
        "    ('france', 'french', 'germany'),\n",
        "    ('london', 'england', 'paris'),\n",
        "    ('cat', 'kitten', 'dog'),\n",
        "    ('rich', 'poor', 'strong')\n",
        "]\n",
        "for a, b, c in analogies:\n",
        "    print(f\"\\nAnalogy: {a}:{b} :: {c}:?\")\n",
        "    results = find_analogy(a, b, c, wv_from_bin)\n",
        "    for word, similarity in results:\n",
        "        print(f\"{word}: {similarity:.3f}\")\n",
        "        assert 0 <= similarity <= 1, f\"Invalid similarity score: {similarity}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7d3dwqzrFin"
      },
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qFpPf0lrFin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d197c20-90df-4ca0-c8d4-69a6370dddf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-26 09:37:45--  https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/classification/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 738844 (722K) [text/plain]\n",
            "Saving to: ‘data/classification/train.txt’\n",
            "\n",
            "\r          data/clas   0%[                    ]       0  --.-KB/s               \rdata/classification 100%[===================>] 721.53K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-02-26 09:37:45 (91.4 MB/s) - ‘data/classification/train.txt’ saved [738844/738844]\n",
            "\n",
            "--2025-02-26 09:37:45--  https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/classification/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94400 (92K) [text/plain]\n",
            "Saving to: ‘data/classification/dev.txt’\n",
            "\n",
            "data/classification 100%[===================>]  92.19K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-02-26 09:37:45 (31.4 MB/s) - ‘data/classification/dev.txt’ saved [94400/94400]\n",
            "\n",
            "--2025-02-26 09:37:45--  https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/classification/test-blind.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 189920 (185K) [text/plain]\n",
            "Saving to: ‘data/classification/test-blind.txt’\n",
            "\n",
            "data/classification 100%[===================>] 185.47K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-02-26 09:37:45 (43.5 MB/s) - ‘data/classification/test-blind.txt’ saved [189920/189920]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2025/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRj8vbrBrFin"
      },
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPdeEDZArFin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e47887a-612f-4165-92f0-c744f7811211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNIGRAM Results:\n",
            "Precision: 0.789\n",
            "Recall: 0.789\n",
            "F1-score: 0.789\n",
            "\n",
            "BIGRAM Results:\n",
            "Precision: 0.721\n",
            "Recall: 0.718\n",
            "F1-score: 0.716\n",
            "\n",
            "GLOVE Results:\n",
            "Precision: 0.769\n",
            "Recall: 0.768\n",
            "F1-score: 0.768\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "def load_data(filepath: str) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"Load text and labels from file\"\"\"\n",
        "    texts, labels = [], []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split('\\t')\n",
        "            if len(line) > 1:\n",
        "                label, text = line\n",
        "                texts.append(text)\n",
        "                labels.append(int(label))\n",
        "            else:\n",
        "                texts.extend(line)\n",
        "    return texts, labels\n",
        "\n",
        "class SentimentClassifier:\n",
        "    def __init__(self, feature_type: str = 'unigram'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feature_type: One of 'unigram', 'bigram', or 'glove'\n",
        "        \"\"\"\n",
        "        self.feature_type = feature_type\n",
        "        self.vectorizer = None\n",
        "        self.classifier = LogisticRegression(random_state=42)\n",
        "        self.wv_from_bin = wv_from_bin\n",
        "\n",
        "    def extract_features(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Extract features based on feature_type\"\"\"\n",
        "        if self.feature_type == 'unigram':\n",
        "            # TODO: Implement unigram feature extraction\n",
        "            if self.vectorizer is None:\n",
        "                self.vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "                return self.vectorizer.fit_transform(texts).toarray()\n",
        "            else:\n",
        "                return self.vectorizer.transform(texts).toarray()\n",
        "        elif self.feature_type == 'bigram':\n",
        "            # TODO: Implement bigram feature extraction\n",
        "            if self.vectorizer is None:\n",
        "                self.vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "                return self.vectorizer.fit_transform(texts).toarray()\n",
        "            else:\n",
        "                return self.vectorizer.transform(texts).toarray()\n",
        "        elif self.feature_type == 'glove':\n",
        "            # TODO: Implement glove feature extraction\n",
        "            # Average word vectors for each text\n",
        "            features = []\n",
        "            for text in texts:\n",
        "                words = text.split()\n",
        "                word_vectors = [self.wv_from_bin[word] for word in words if word in self.wv_from_bin]\n",
        "\n",
        "                if len(word_vectors) > 0:\n",
        "                    features.append(np.mean(word_vectors, axis=0))\n",
        "                else:\n",
        "                    features.append(np.zeros(200))\n",
        "            return np.array(features)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid feature type: {self.feature_type}\")\n",
        "\n",
        "    def train(self, train_texts: List[str], train_labels: List[int]) -> None:\n",
        "        \"\"\"Train sentiment classifier\"\"\"\n",
        "        # TODO: Implement training\n",
        "        features = self.extract_features(train_texts)\n",
        "        self.classifier.fit(features, train_labels)\n",
        "\n",
        "    def predict(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Predict sentiment labels\"\"\"\n",
        "        # TODO: Implement prediction\n",
        "        features = self.extract_features(texts)\n",
        "        return self.classifier.predict(features)\n",
        "\n",
        "    def evaluate(self, texts: List[str], labels: List[int]) -> Dict:\n",
        "        \"\"\"Evaluate classifier performance\"\"\"\n",
        "        predictions = self.predict(texts)\n",
        "        return classification_report(labels, predictions, output_dict=True)\n",
        "\n",
        "# Train and evaluate models with different features\n",
        "train_texts, train_labels = load_data('data/classification/train.txt')\n",
        "dev_texts, dev_labels = load_data('data/classification/dev.txt')\n",
        "\n",
        "results = {}\n",
        "for feature_type in ['unigram', 'bigram', 'glove']:\n",
        "    classifier = SentimentClassifier(feature_type)\n",
        "    classifier.train(train_texts, train_labels)\n",
        "    results[feature_type] = classifier.evaluate(dev_texts, dev_labels)\n",
        "\n",
        "# Print results\n",
        "for feature_type, metrics in results.items():\n",
        "    print(f\"\\n{feature_type.upper()} Results:\")\n",
        "    print(f\"Precision: {metrics['weighted avg']['precision']:.3f}\")\n",
        "    print(f\"Recall: {metrics['weighted avg']['recall']:.3f}\")\n",
        "    print(f\"F1-score: {metrics['weighted avg']['f1-score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIAHJjTLrFin"
      },
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     |  0.789    |   0.789  |    0.789  |\n",
        "| bigram      |  0.721      | 0.718    |   0.716    |\n",
        "| GloVe       |      0.769  |   0.768   |   0.768    |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results suggest that unigram features perform the best in terms of precision, recall, and F1-score, indicating that simple word-level features capture the sentiment well in this task.\n",
        "\n",
        "In comparison, bigram features show a slight drop in performance, which could be due to the sparsity of meaningful bigrams in the dataset or the introduction of noise, making it harder for the model to generalize.\n",
        "\n",
        "Finally, the GloVe-based model performs similarly to unigrams, which might be because the averaging of word embeddings doesn't capture enough contextual information for significant improvement over unigram features."
      ],
      "metadata": {
        "id": "SOtCOotpUSPa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdZl-3I-rFin"
      },
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
        "\n",
        "def load_data(filepath: str) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"Load text and labels from file\"\"\"\n",
        "    texts, labels = [], []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split('\\t')\n",
        "            if len(line) > 1:\n",
        "                label, text = line\n",
        "                texts.append(text)\n",
        "                labels.append(int(label))\n",
        "            else:\n",
        "                texts.extend(line)\n",
        "    return texts, labels\n",
        "\n",
        "class SentimentClassifier:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.vectorizer = None\n",
        "        self.classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "    def train_bpe_tokenizer(self, texts: List[str], vocab_size: int = 10000):\n",
        "        tokenizer = Tokenizer(models.BPE())\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "        tokenizer.decoder = decoders.BPEDecoder()\n",
        "\n",
        "        trainer = trainers.BpeTrainer(vocab_size=vocab_size)\n",
        "        tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def extract_features(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Extract features using BPE tokenization\"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            self.train_bpe_tokenizer(texts)\n",
        "\n",
        "        # Tokenize texts\n",
        "        tokenized_texts = [self.tokenizer.encode(text).tokens for text in texts]\n",
        "\n",
        "        # Convert tokens to space-separated strings for CountVectorizer\n",
        "        tokenized_texts = [\" \".join(tokens) for tokens in tokenized_texts]\n",
        "\n",
        "        if self.vectorizer is None:\n",
        "            self.vectorizer = CountVectorizer(ngram_range=(1, 1), max_df=500)\n",
        "            return self.vectorizer.fit_transform(tokenized_texts).toarray()\n",
        "        else:\n",
        "            return self.vectorizer.transform(tokenized_texts).toarray()\n",
        "\n",
        "    def train(self, train_texts: List[str], train_labels: List[int]) -> None:\n",
        "        \"\"\"Train sentiment classifier\"\"\"\n",
        "        features = self.extract_features(train_texts)\n",
        "        self.classifier.fit(features, train_labels)\n",
        "\n",
        "    def predict(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Predict sentiment labels\"\"\"\n",
        "        features = self.extract_features(texts)\n",
        "        return self.classifier.predict(features)\n",
        "\n",
        "    def evaluate(self, texts: List[str], labels: List[int]) -> Dict:\n",
        "        \"\"\"Evaluate classifier performance\"\"\"\n",
        "        predictions = self.predict(texts)\n",
        "        return classification_report(labels, predictions, output_dict=True)\n",
        "\n",
        "# Train and evaluate models with different features\n",
        "train_texts, train_labels = load_data('data/classification/train.txt')\n",
        "dev_texts, dev_labels = load_data('data/classification/dev.txt')\n",
        "\n",
        "results = {}\n",
        "classifier = SentimentClassifier()\n",
        "classifier.train(train_texts, train_labels)\n",
        "results = classifier.evaluate(dev_texts, dev_labels)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {results['weighted avg']['precision']:.3f}\")\n",
        "print(f\"Recall: {results['weighted avg']['recall']:.3f}\")\n",
        "print(f\"F1-score: {results['weighted avg']['f1-score']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6OdUW-5qOCd",
        "outputId": "61709658-b1b5-4235-87b6-82a23964544a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.805\n",
            "Recall: 0.805\n",
            "F1-score: 0.805\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
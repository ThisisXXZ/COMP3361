{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Section 1: Basic Decoding Algorithms","metadata":{"id":"tfwAdXU4urVp"}},{"cell_type":"code","source":"pip install transformers datasets evaluate markdownify","metadata":{"id":"EUc1HaH0J1M4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"set device and random seeds\"\"\"\n\n######################################################\n#  The following helper functions are given to you.\n######################################################\n\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn.functional as F\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device: {device}')\n\ndef set_seed(seed=19260817):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed()","metadata":{"id":"O7WLNZn_9d7n","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"load datasets\"\"\"\n\n######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('Ximing/ROCStories')\ntrain_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n\nprint(train_data[0])","metadata":{"id":"dJrHDSAIJ7ku","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"prepare evaluation\"\"\"\n\n######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom evaluate import load\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\n\nperplexity_scorer = load(\"perplexity\", module_type=\"metric\")\ncola_model_name = \"textattack/roberta-base-CoLA\"\ncola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\ncola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n\ndef batchify(data, batch_size):\n    assert batch_size > 0\n\n    batch = []\n    for item in data:\n        # Yield next batch\n        if len(batch) == batch_size:\n            yield batch\n            batch = []\n\n        batch.append(item)\n\n    # Yield last un-filled batch\n    if len(batch) != 0:\n        yield batch","metadata":{"id":"9H4QKWbtJ9MW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"set up evaluation metric\"\"\"\n\n######################################################\n#  The following helper code is given to you.\n######################################################\n\ndef compute_perplexity(texts, model='gpt2', batch_size=8):\n    score = perplexity_scorer.compute(predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model)\n    return score['mean_perplexity']\n\n\ndef compute_fluency(texts, batch_size=8):\n  scores = []\n  for b_texts in batchify(texts, batch_size):\n    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n      logits = cola_model(**inputs).logits\n      probs = logits.softmax(dim=-1)\n      scores.extend(probs[:, 1].tolist())\n  return sum(scores) / len(scores)\n\n\ndef compute_diversity(texts):\n    unigrams, bigrams, trigrams = [], [], []\n    total_words = 0\n    for gen in texts:\n        o = gen.split(' ')\n        total_words += len(o)\n        for i in range(len(o)):\n            unigrams.append(o[i])\n        for i in range(len(o) - 1):\n            bigrams.append(o[i] + '_' + o[i + 1])\n        for i in range(len(o) - 2):\n            trigrams.append(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n    return len(set(unigrams)) / len(unigrams), len(set(bigrams)) / len(bigrams), len(set(trigrams)) / len(trigrams)\n\n\ndef evaluate(generations, experiment):\n  generations = [_ for _ in generations if _ != '']\n  perplexity = compute_perplexity(generations)\n  fluency = compute_fluency(generations)\n  diversity = compute_diversity(generations)\n  print(experiment)\n  print(f'perplexity = {perplexity:.2f}')\n  print(f'fluency = {fluency:.2f}')\n  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n  print()\n\ndebug_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\nevaluate(debug_sents, 'debugging run')","metadata":{"id":"NUuU65RsJ_GI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"load model and tokenizer\"\"\"\n\n######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_name = 'gpt2'\ntokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\nmodel = GPT2LMHeadModel.from_pretrained(model_name).to(device)\nmodel.eval()","metadata":{"id":"hxU5PdFtKBJO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this section, you will implement a few basic decoding algorithms:\n1. Greedy decoding\n2. Vanilla sampling\n3. Temperature sampling\n4. Top-k sampling\n5. Top-p sampling\n\nWe have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\nYou will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n\n**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**","metadata":{"id":"9zXVylNcKFOJ"}},{"cell_type":"code","source":"\"\"\"decode main wrapper function\"\"\"\n\n######################################################\n#  The following helper code is given to you.\n######################################################\n\ndef decode(prompts, max_len, method, **kwargs):\n  encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n  input_ids = encodings_dict['input_ids'].to(device)\n  attention_mask = encodings_dict['attention_mask'].to(device)\n\n  model_kwargs = {'attention_mask': attention_mask, \"use_cache\": False}\n  batch_size, input_seq_len = input_ids.shape\n\n  unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n\n  for step in range(max_len):\n    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n    cache_position = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n    model_kwargs[\"cache_position\"] = cache_position\n    with torch.no_grad():\n      outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n\n    if step == 0:\n      last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n      next_token_logits = outputs.logits[range(batch_size), last_non_masked_idx, :]\n    else:\n      next_token_logits = outputs.logits[:, -1, :]\n\n    log_prob = F.log_softmax(next_token_logits, dim=-1)\n\n    if method == 'greedy':\n      next_tokens = greedy(next_token_logits)\n    elif method == 'sample':\n      next_tokens = sample(next_token_logits)\n    elif method == 'temperature':\n      next_tokens = temperature(next_token_logits, t=kwargs.get('t', 0.8))\n    elif method == 'topk':\n      next_tokens = topk(next_token_logits, k=kwargs.get('k', 20))\n    elif method == 'topp':\n      next_tokens = topp(next_token_logits, p=kwargs.get('p', 0.7))\n\n    # finished sentences should have their next token be a padding token\n    next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n\n    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n    model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder)\n\n    # if eos_token was found in one sentence, set sentence to finished\n    unfinished_sequences = unfinished_sequences.mul((next_tokens != tokenizer.eos_token_id).long())\n\n    if unfinished_sequences.max() == 0:\n      break\n\n  response_ids = input_ids[:, input_seq_len:]\n  response_text = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in response_ids]\n\n  return response_text","metadata":{"id":"fO8x8XJaKEwF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"debug helper code\"\"\"\n\n######################################################\n#  The following helper code is given to you.\n######################################################\n\n# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\ndev_prompts = [dev_data[0]['prompt']] * 10\n\ndef print_generations(prompts, generations):\n  for prompt, generation in zip(prompts, generations):\n    print(f'{[prompt]} ==> {[generation]}')","metadata":{"id":"c17DrQISKNzI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.1 Greedy Decoding","metadata":{"id":"9ryFGrlRSXYn"}},{"cell_type":"code","source":"def greedy(next_token_logits):\n  '''\n  inputs:\n  - next_token_logits: Tensor(size = (B, V), dtype = float)\n  outputs:\n  - next_tokens: Tensor(size = (B), dtype = long)\n  '''\n\n  # TODO: compute `next_tokens` from `next_token_logits`.\n  # Hint: use torch.argmax()\n  next_tokens = torch.argmax(next_token_logits, dim=1)\n\n  return next_tokens\n\n\ngenerations = decode(dev_prompts, max_len=20, method='greedy')\nprint_generations(dev_prompts, generations)","metadata":{"id":"xH0wBhy2SZa-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2 Vanilla Sampling and Temperature Sampling","metadata":{"id":"aGKL_31VJNw1"}},{"cell_type":"code","source":"def sample(next_token_logits):\n  '''\n  inputs:\n  - next_token_logits: Tensor(size = (B, V), dtype = float)\n  outputs:\n  - next_tokens: Tensor(size = (B), dtype = long)\n  '''\n\n  # TODO: compute the probabilities `probs` from the logits.\n  # Hint: `probs` should have size (B, V)\n  probs = torch.softmax(next_token_logits, dim=1)\n\n  # TODO: compute `next_tokens` from `probs`.\n  # Hint: use torch.multinomial()\n  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\n  return next_tokens\n\n\nset_seed()\ngenerations = decode(dev_prompts, max_len=20, method='sample')\nprint_generations(dev_prompts, generations)","metadata":{"id":"PjrTLKj2JR5b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def temperature(next_token_logits, t):\n  '''\n  inputs:\n  - next_token_logits: Tensor(size = (B, V), dtype = float)\n  - t: float\n  outputs:\n  - next_tokens: Tensor(size = (B), dtype = long)\n  '''\n\n  # TODO: compute the probabilities `probs` from the logits, with temperature applied.\n  probs = torch.softmax(next_token_logits / t, dim=1)\n\n  # TODO: compute `next_tokens` from `probs`.\n  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\n  return next_tokens\n\n\nset_seed()\ngenerations = decode(dev_prompts, max_len=20, method='temperature', t=0.8)\nprint_generations(dev_prompts, generations)","metadata":{"id":"25Su03uzJb_Z","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.3 Top-k Sampling","metadata":{"id":"bPqWUJiFK43O"}},{"cell_type":"code","source":"def topk(next_token_logits, k):\n  '''\n  inputs:\n  - next_token_logits: Tensor(size = (B, V), dtype = float)\n  - k: int\n  outputs:\n  - next_tokens: Tensor(size = (B), dtype = long)\n  '''\n\n  # TODO: Keep only top-k tokens with highest probabilities.\n  # Hint: use torch.topk()\n  topk_logits, topk_indices = torch.topk(next_token_logits, k, dim=1)\n\n  # Create a mask to zero out all logits not in top-k\n  indices_to_remove = next_token_logits < topk_logits[:, -1].unsqueeze(1)\n\n  # Mask the logits\n  next_token_logits[indices_to_remove] = float('-inf')\n\n  # TODO: Sample from the masked logits\n  probs = torch.softmax(next_token_logits, dim=1)\n  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\n  return next_tokens\n\n\nset_seed()\ngenerations = decode(dev_prompts, max_len=20, method='topk', k=20)\nprint_generations(dev_prompts, generations)","metadata":{"id":"yjXyfL4GK3mK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.4 Top-p Sampling","metadata":{"id":"NSjMWNFEy_cC"}},{"cell_type":"code","source":"def topp(next_token_logits, p):\n  '''\n  inputs:\n  - next_token_logits: Tensor(size = (B, V), dtype = float)\n  - p: float\n  outputs:\n  - next_tokens: Tensor(size = (B), dtype = long)\n  '''\n\n  # TODO: Sort the logits in descending order, and compute\n  # the cumulative probabilities `cum_probs` on the sorted logits\n  sorted_logits, sorted_indices = torch.sort(next_token_logits, dim=1, descending=True)\n  sorted_probs = torch.softmax(sorted_logits, dim=1)\n  cum_probs = torch.cumsum(sorted_probs, dim=1)\n\n  # Create a mask to zero out all logits not in top-p\n  sorted_indices_to_remove = cum_probs > p\n  sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n  sorted_indices_to_remove[:, 0] = 0\n  # Restore mask to original indices\n  indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n\n  # Mask the logits\n  next_token_logits[indices_to_remove] = float('-inf')\n\n  # TODO: Sample from the masked logits\n  probs = torch.softmax(next_token_logits, dim=1)\n  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\n  return next_tokens\n\n\nset_seed()\ngenerations = decode(dev_prompts, max_len=20, method='topp', p=0.7)\nprint_generations(dev_prompts, generations)","metadata":{"id":"Oq1ZwVVxzApa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.5: Evaluation\n\nRun the following cell to obtain the evaluation results, which you should include in your writeup.\nAlso don't forget to answer the questions.","metadata":{"id":"MGfhvyy0Ka8i"}},{"cell_type":"code","source":"prompts = [item['prompt'] for item in test_data][:10]\nGENERATIONS_PER_PROMPT = 10\nMAX_LEN = 100\n\nfor experiment in ['greedy', 'sample', 'temperature', 'topk', 'topp']:\n  generations = []\n  for prompt in tqdm(prompts):\n    generations += decode([prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment)\n  evaluate(generations, experiment)","metadata":{"id":"tGyyCSgOKbgw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Discussion\n- Q1.1: In greedy decoding, what do you observe when generating 10 times from the test prompt?\n    - All 10 generations are exactly identical. This is because the model consistently selects the token with the highest probability at each step without introducing any randomness.","metadata":{"id":"oUc_IncHKkRF"}},{"cell_type":"markdown","source":"- Q1.2: In vanilla sampling, what do you observe when generating 10 times from the test prompt?\n    - The 10 generations are highly diverse, with each output being different in content, structure and even coherence. It seems to me that most of the continuations bear little meaningful connection to the given prompt.","metadata":{"id":"SoDRS10iKzzc"}},{"cell_type":"markdown","source":"- Q1.3: In temperature sampling, play around with the value of temperature $t$. Which value of $t$ makes it equivalent to greedy decoding? Which value of $t$ makes it equivalent to vanilla sampling?\n    - When $t\\to 0$, the probability distribution becomes really sharp and concentrates nearly all the probability mass on the highest logit. In this case, the sampling almost always selects the top token, which behaves like **greedy encoding**.\n    - When $t=1$, the raw probabilities are not scaled, which makes temperature sampling behave identically to **vanilla sampling.**","metadata":{"id":"ghPXsI0YK1Db"}},{"cell_type":"markdown","source":"- Q1.4: In top-$p$ sampling, play around with the value of $p$. Which value of $p$ makes it equivalent to greedy decoding? Which value of $p$ makes it equivalent to vanilla sampling?\n    - When $p\\to 0$, only the single probable token is selected (we can observe from the code that ``sorted_indices_to_remove[:, 0] = 0`` ensures that the token with highest probability is never removed), which makes top-p sampling equivalent to **greedy decoding**.\n    - When $p=1$, the full probability mass is retained, thus the sampling behaves equivalently to **vanilla sampling**.","metadata":{"id":"YE16fo91K2VM"}},{"cell_type":"markdown","source":"- Q1.5: In top-k sampling, play around with the value of k. Which value of k makes it equivalent to greedy decoding? Which value of $k$ makes it equivalent to vanilla sampling?\n    - When $k=1$, the model always selects the token with the highest probability. There is no randomness left and it behaves exactly like **greedy decoding**.\n    - When $k=|V|=50257$, the model retains all tokens for sampling. This is equivalent to **vanilla sampling**.","metadata":{"id":"-wMu-w_FK3z5"}},{"cell_type":"markdown","source":"- Q1.6: Report the evaluation metrics (perplexity, fluency, diversity) of all 5 decoding methods. Which methods have the best and worst perplexity? Fluency? Diversity?\n\nI organized the evaluation metrics into a table.\n\n|Method|Preplexity|Fluency|Diversity (1,2,3-gram)|\n|---|---|---|---|\n|Greedy|2.08|0.78|0.01, 0.02, 0.03|\n|Sample|70.58|0.34|0.43, 0.90, 0.99|\n|Temperature|16.61|0.68|0.33, 0.79, 0.95|\n|Top-k|12.24|0.74|0.26, 0.75, 0.96|\n|Top-p|12.40|0.72|0.29, 0.76, 0.96|\n\n\n* **Perplexity**: Greedy decoding has the best perplexity (2.08), and vanilla sampling has the worst (70.58)\n* **Fluency**: Greedy decoding has the best fluency (0.78), and vanilla sampling has the worst (0.34)\n* **Diversity**: Vanilla sampling has the best diversity (0.43, 0.90, 0.99), and greedy decoding has the worst (0.01, 0.02, 0.03)\n\n**Greedy decoding** achieves the best perplexity and fluency but produces almost no diversity. **Vanilla sampling** offers the highest diversity but suffers from poor perplexity and fluency.\n\n**Temperature**, **top-k**, and **top-p sampling** strike a better balance, with top-k slightly outperforming the others in terms of fluency and perplexity while maintaining good diversity.","metadata":{"id":"Q0ASl7jxoDR-"}},{"cell_type":"markdown","source":"## Section 2: Applying Large Language Models to Few Shot Math Reasoning","metadata":{"id":"Njf8q19ZJ4cJ"}},{"cell_type":"code","source":"pip install -q vllm bitblas # restart runtime session after install requirements","metadata":{"id":"aMVRgAIkj0Ve","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"HF_HOME\"] = \"/content/.cache/huggingface\" # set the cache directory to personal disk to avoid frequent downloads","metadata":{"id":"iQvzOL51kwNO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nmodel_id = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\nllm = LLM(model=model_id, enforce_eager=True, quantization=\"gptq\")","metadata":{"id":"bX7-W5OEko_Y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom openai import OpenAI\nfrom transformers import AutoTokenizer\n\nclass VLLMClient:\n    def __init__(self, model_id, **kwargs):\n      self.model_id = model_id\n\n    def __call__(self, prompt: str, **kwargs):\n      response = llm.generate(\n            prompts=prompt,\n            sampling_params=SamplingParams(\n                temperature=kwargs.get(\"temperature\", 0.2),\n                max_tokens=kwargs.get(\"max_tokens\", 256),\n                stop=[\"###\"]\n            )\n      )\n      return response[0].outputs[0].text\nmodel = VLLMClient(model_id)\nmodel(\"San Francisco is a\", max_tokens=42)","metadata":{"id":"0pxe1n_slCJx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nGSM_EXAMPLARS = [\n    {\n        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"cot_answer\": \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\",\n        \"short_answer\": \"6\"\n    },\n    {\n        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n        \"cot_answer\": \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\",\n        \"short_answer\": \"5\"\n    },\n    {\n        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"cot_answer\": \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\",\n        \"short_answer\": \"39\"\n    },\n    {\n        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"cot_answer\": \"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\",\n        \"short_answer\": \"8\"\n    },\n    {\n        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n        \"cot_answer\": \"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\",\n        \"short_answer\": \"9\"\n    },\n    {\n        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"cot_answer\": \"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\",\n        \"short_answer\": \"29\"\n    },\n    {\n        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"cot_answer\": \"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\",\n        \"short_answer\": \"33\"\n    },\n    {\n        \"question\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n        \"cot_answer\": \"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\",\n        \"short_answer\": \"8\"\n    }\n]","metadata":{"id":"Z9k51qESwRzp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p data\n!wget -q -O data/gsm8k.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/gsm8k.jsonl\n!wget -q -O data/simpleqa.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/simpleqa.jsonl\n!wget -q -O data/math.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/math.jsonl\n!wget -q -O data/gaia.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/gaia.jsonl","metadata":{"id":"eyR0w4d62rf7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nimport json\n\ndef load_eval_data(task):\n    with open(f\"data/{task}.jsonl\", \"r\") as f:\n        return [json.loads(line) for line in f]\n\ntasks = [\"gsm8k\", \"simpleqa\", \"math\", \"gaia\"]\n\nfor task in tasks:\n    print(f\"Example of {task} dataset:\")\n    print(json.dumps(load_eval_data(task)[0], indent=4))","metadata":{"id":"PTh2lX8V2l8j","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nimport os\nimport datetime\nimport threading\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nAPPEND_ANSWER_LOCK = threading.Lock()\n\ndef append_answer(entry: dict, jsonl_file: str) -> None:\n    jsonl_file = Path(jsonl_file)\n    jsonl_file.parent.mkdir(parents=True, exist_ok=True)\n    with APPEND_ANSWER_LOCK, open(jsonl_file, \"a\", encoding=\"utf-8\") as fp:\n        fp.write(json.dumps(entry) + \"\\n\")\n    assert os.path.exists(jsonl_file), \"File not found!\"\n\ndef answer_single_question(example, agent, answers_file, action_type):\n    augmented_question = example[\"question\"]\n    if example[\"source\"] == \"SimpleQA\":\n        augmented_question += \" Answer with only the final number.\"\n    if example[\"source\"] == \"MATH\":\n        augmented_question += \" Write code, not latex.\"\n\n    start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    answer = str(agent.run(augmented_question))\n\n    end_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    annotated_example = {\n        \"model_id\": model.model_id,\n        \"agent_action_type\": action_type,\n        \"question\": augmented_question,\n        \"original_question\": example[\"question\"],\n        \"answer\": answer,\n        \"true_answer\": example[\"true_answer\"],\n        \"source\": example[\"source\"],\n        \"start_time\": start_time,\n        \"end_time\": end_time,\n    }\n    append_answer(annotated_example, answers_file)\n\ndef answer_questions(\n    task,\n    agent,\n    action_type: str = \"vanilla\",\n    answers_file: str = None,\n    parallel_workers: int = 4,\n):\n    eval_data = load_eval_data(task)\n\n    print(f\"Starting processing and writing output to '{answers_file}'\")\n\n    answered_questions = []\n    if os.path.exists(answers_file):\n        with open(answers_file, \"r\") as f:\n            for line in f:\n                answered_questions.append(json.loads(line)[\"original_question\"])\n\n    examples_todo = [example for example in eval_data if example[\"question\"] not in answered_questions]\n\n\n    for i, example in enumerate(tqdm(examples_todo)):\n        answer_single_question(example, agent, answers_file, action_type)","metadata":{"id":"wypUrNwJ4s0W","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget -q -O eval_utils.py https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/evaluate.py","metadata":{"id":"qxayWofTGe0b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from eval_utils import score_answers, extract_numbers\n\nclass FewShotReasoner:\n    def __init__(self, model, n_shots):\n        self.model = model\n        self.n_shots = n_shots\n\n    def run(self, task):\n        prompt = self.build_input(task)\n        raw_output = self.model(prompt, max_tokens=64)\n        return extract_numbers(raw_output.strip())\n\n    def build_input(self, task):\n        prompt = \"Answer the following questions.\\n\"\n        for example in GSM_EXAMPLARS[:self.n_shots]:\n            prompt += f\"Question: {example['question']}\\nAnswer: {example['short_answer']}\\n###\\n\"\n        prompt += f\"Question: {task}\\nAnswer: \"\n        return prompt\n\n\n\ndef run_gsm8k_fewshot_reasoner(task=\"gsm8k\", model_id=model_id, action_type=\"vanilla\"):\n    reasoner_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n    chat_agent = FewShotReasoner(model, 8)\n    answer_questions(task, chat_agent, action_type, reasoner_answers_file)\n    df = score_answers([reasoner_answers_file])\n    print(df)\n\nrun_gsm8k_fewshot_reasoner()","metadata":{"id":"VYPInNG85oQr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from eval_utils import extract_numbers\n\nclass FewShotCoTReasoner:\n    def __init__(self, model, n_shots):\n        self.model = model\n        self.n_shots = n_shots\n\n    def run(self, task):\n        prompt = self.build_input(task)\n        cot_output = self.model(prompt)\n        return extract_numbers(cot_output.strip())[-1]\n\n    def build_input(self, task):\n        prompt = \"Answer the following grade-school math word problems step-by-step. Show intermediate calculations, and be careful with percentages, time units, discounts, and compound conditions. \\n\"\n        for example in GSM_EXAMPLARS[:self.n_shots]:\n            prompt += f\"Question: {example['question']}\\nAnswer: {example['cot_answer']}\\n###\\n\"\n        prompt += f\"Question: {task}\\nAnswer: \"\n        return prompt\n\n\ndef run_gsm8k_fewshot_cot_reasoner(task=\"gsm8k\", model_id=model_id, action_type=\"vanilla\"):\n    reasoner_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}__cot.jsonl\"\n    chat_agent = FewShotCoTReasoner(model, 8)\n    answer_questions(task, chat_agent, action_type, reasoner_answers_file)\n    df = score_answers([reasoner_answers_file])\n    print(df)\n\nrun_gsm8k_fewshot_cot_reasoner()","metadata":{"id":"JWT7nVRSD-Ao","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 3:Building and Evaluating LLM Agents","metadata":{"id":"AIY-UV7dJbfm"}},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom transformers import AutoTokenizer\nclass ChatAgent:\n    def __init__(self, model):\n        self.model = model\n        self.tokenizer = AutoTokenizer.from_pretrained(model.model_id)\n\n    def run(self, task):\n        prompt = self.build_input(task)\n        return self.model(prompt=prompt)\n\n    def build_input(self, task):\n        messages = [{\"role\": \"user\", \"content\": task}]\n        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\naction_type=\"vanilla\"\nchat_agent_answers_files = {}\nfor task in [\"simpleqa\", \"math\", \"gaia\"]:\n    chat_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n    chat_agent = ChatAgent(model)\n    answer_questions(task, chat_agent, action_type, chat_agent_answers_file, parallel_workers=8)\n    chat_agent_answers_files[task] = chat_agent_answers_file\nscore_answers(chat_agent_answers_files.values())","metadata":{"id":"WTRzKmQrHhSw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom io import StringIO\nimport sys\nimport inspect\nfrom typing import Any, Dict, Union, Optional, Tuple, List\n\nclass Tool:\n    name: str\n    description: str\n    inputs: Dict[str, Dict[str, Union[str, type, bool]]]\n    output_type: str\n\n    def __init__(self, *args, **kwargs):\n        self.is_initialized = False\n\n    def forward(self, *args, **kwargs):\n        return NotImplementedError(\"Write this method in your subclass of `Tool`.\")\n\n    def __call__(self, *args, **kwargs):\n        outputs = self.forward(*args, **kwargs)\n        return outputs\n\n    def to_dict(self) -> dict:\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": self.name,\n                \"description\": self.description,\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": self.inputs,\n                    \"required\": list(self.inputs.keys()),\n                },\n            },\n            \"strict\": True\n        }\n","metadata":{"id":"TeTw_pg6OAKL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom typing import Any\n\nclass FinalAnswerTool(Tool):\n    name = \"final_answer\"\n    description = \"Use this tool to return your final answer to the user. Only use when you have high confidence in your answer.\"\n    inputs = {\"answer\": {\"type\": \"any\", \"description\": \"The final answer to the problem\"}}\n    output_type = \"any\"\n\n    def forward(self, answer: Any) -> Any:\n        return answer","metadata":{"id":"IEHWM_jPOn6U","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfinal_answer_tool = FinalAnswerTool()\nfinal_answer_tool(\"Hello, world!\")\n\ntools = [\n    final_answer_tool.to_dict()\n]\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What year was the municipality of Ramiriqu\\u00ed, Boyac\\u00e1, Colombia, founded?  Answer with only the final number.\"}\n]\ntokenizer = AutoTokenizer.from_pretrained(model.model_id)\nprint(\"=\"*10 + \"Input Prompt\" + \"=\"*10 + \"\\n\")\ninput_prompt = tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=True)\nprint(\"\\n\\n\" + input_prompt)\nprint(\"=\"*10 + \"Response\" + \"=\"*10 + \"\\n\")\nresponse = model(input_prompt)\nprint(\"\\n\\n\" + response)","metadata":{"id":"7GdZc8TUOs0N","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GoogleSearchTool(Tool):\n    name = \"web_search\"\n    description = \"\"\"Search the web for current or factual information using a query. Use this when you do not know the answer or need updated information.\"\"\"\n    inputs = {\n        \"query\": {\"type\": \"string\", \"description\": \"The search query to perform.\"},\n    }\n    output_type = \"string\"\n\n    def __init__(self, provider: str = \"serper\"):\n        super().__init__()\n        self.provider = provider\n        if provider != \"serper\":\n            raise ValueError(f\"Unsupported provider: {provider}\")\n\n        self.api_key = os.getenv(\"SERPER_API_KEY\")\n        if not self.api_key:\n            raise EnvironmentError(\"SERPER_API_KEY environment variable is not set.\")\n\n    def forward(self, query: str) -> str:\n        import requests\n\n        # Register Google Search Api through https://serper.dev/\n        # Use the organic key from search results to build this tool.\n\n        url = \"https://google.serper.dev/search\"\n        headers = {\n            \"X-API-KEY\": self.api_key,\n            \"Content-Type\": \"application/json\"\n        }\n        payload = {\n            \"q\": query\n        }\n\n        response = requests.post(url, headers=headers, json=payload)\n\n        if response.status_code != 200:\n            raise RuntimeError(f\"Serper API request failed: {response.status_code} - {response.text}\")\n\n        data = response.json()\n        results = data.get(\"organic\", [])\n\n        if not results:\n            return \"No search results found.\"\n\n        formatted_results = []\n        for i, result in enumerate(results[:3], 1):\n            snippet = result.get(\"snippet\", \"\")\n            link = result.get(\"link\", \"\")\n            formatted_results.append(f\" {i}. Link. {link} About. {snippet}\")\n\n        return \"||\".join(formatted_results)","metadata":{"id":"KEcoTB-pQkxg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"SERPER_API_KEY\"] = \"dbcf96fefaca8c2235ab8e1d65651e7211ac9b41\" # remember to remove your key for submission\ngoogle_search_tool = GoogleSearchTool()\ngoogle_search_tool(\"Space Exploration Technologies Corp.\")","metadata":{"id":"FxstLV5AQuIf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install markdownify","metadata":{"id":"0RVA8fEoZrmv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VisitWebpageTool(Tool):\n    name = \"visit_webpage\"\n    description = (\n        \"Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages.\"\n    )\n    inputs = {\n        \"url\": {\n            \"type\": \"string\",\n            \"description\": \"The url of the webpage to visit.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, max_output_length: int = 10000):\n        super().__init__()\n        self.max_output_length = max_output_length\n\n    def forward(self, url: str) -> str:\n        try:\n            import re\n            import requests\n            from markdownify import markdownify\n            from requests.exceptions import RequestException\n        except ImportError as e:\n            raise ImportError(\n                \"You must install packages `markdownify` and `requests` to run this tool.\"\n            ) from e\n\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n\n            html = response.text\n            markdown_content = markdownify(html)\n\n            # Optional: normalize excessive line breaks\n            cleaned = re.sub(r'\\n{3,}', '\\n\\n', markdown_content.strip())\n\n            return cleaned[:self.max_output_length]\n\n        except requests.exceptions.Timeout:\n            return \"The request timed out. Please try again later or check the URL.\"\n        except RequestException as e:\n            return f\"Error fetching the webpage: {str(e)}\"\n        except Exception as e:\n            return f\"An unexpected error occurred: {str(e)}\"","metadata":{"id":"7_dtjlhJQo4q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visit_page_tool = VisitWebpageTool()\nvisit_page_tool(\"https://www.spacex.com/\")","metadata":{"id":"cMj8Ftq1Rn99","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom typing import List, Dict, Any, Callable, Optional\n\nsystem_prompt = \"\"\"\nYou are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.\nTo do so, you have been given access to some tools. \n\nYou need to think step-by-step. Explain your reasoning before using a tool.\n\nHere are the rules you should always follow to solve your task:\n1. Always call a tool when solving a task. Wrap the tool call inside <tool_call>...</tool_call>.\n2. Use actual values in tool calls—don’t use variable names.\n3. Only call a tool if you really need it. If you can answer without it, do that instead.\n4. Never repeat a tool call with the same input.\n5. After doing a web search, only visit the URLs you get from the search. Don’t make up or guess any links.\"\"\"\n\n\nclass ToolCallAgent:\n    \"\"\"\n    A minimal agent that uses a language model to interact with tools\n    based on the ToolCallingAgent pattern.\n    \"\"\"\n    def __init__(\n        self,\n        model: Callable,\n        tools: List[Tool],\n        max_steps: int = 5,\n        final_answer_tool_name: str = \"final_answer\",\n        print_log: bool = True\n    ):\n        self.model = model\n        self.tokenizer = AutoTokenizer.from_pretrained(model.model_id)\n        self.tools_definitions = [tool.to_dict() for tool in tools]\n        self.tool_executors = {tool.name: tool for tool in tools}\n        self.max_steps = max_steps\n        self.final_answer_tool_name = final_answer_tool_name\n\n        self.log_func = lambda x: print(x) if print_log else lambda _: None\n\n        # Validate that the final answer tool has an executor\n        if self.final_answer_tool_name not in self.tool_executors:\n            raise ValueError(f\"Executor for final answer tool '{self.final_answer_tool_name}' not found in tool_executors.\")\n\n    @staticmethod\n    def fix_closing_tags(text: str) -> str:\n        \"\"\"A compromise, and I don't know why the model failed to append the closing tag\"\"\"\n        result = []\n        i = 0\n        while i < len(text):\n            start_tag = text.find(\"<tool_call>{\", i)\n            if start_tag == -1:\n                result.append(text[i:])\n                break\n    \n            result.append(text[i:start_tag])\n            brace_start = text.find(\"{\", start_tag)\n            if brace_start == -1:\n                result.append(text[start_tag:])\n                break\n    \n            brace_count = 0\n            end = brace_start\n            while end < len(text):\n                if text[end] == '{':\n                    brace_count += 1\n                elif text[end] == '}':\n                    brace_count -= 1\n                    if brace_count == 0:\n                        break\n                end += 1\n    \n            if brace_count != 0:\n                result.append(text[start_tag:])\n                break\n    \n            json_block = text[start_tag:end + 1]\n            result.append(json_block)\n    \n            remainder = text[end + 1:]\n            if not remainder.lstrip().startswith(\"</tool_call>\"):\n                result.append(\"</tool_call>\")\n                i = end + 1\n            else:\n                close_index = remainder.find(\"</tool_call>\")\n                result.append(remainder[:close_index + len(\"</tool_call>\")])\n                i = end + 1 + close_index + len(\"</tool_call>\")\n    \n        return ''.join(result)\n    \n    def _execute_tool(self, tool_name: str, parsed_args: Any) -> str:\n        executor = self.tool_executors[tool_name]\n        try:\n            # self.log_func(f\"[INFO] Executing tool '{tool_name}' with args: {parsed_args}\")\n            tool_result = executor(**parsed_args)\n            self.log_func(f\"[INFO] Tool '{tool_name}' executed.\")\n            # The final_answer tool itself returns the value, we don't stringify it here yet\n            return tool_result\n        except Exception as e:\n            print(f\"[ERROR] Error executing tool '{tool_name}' with args {parsed_args}: {e}\")\n            # Return the error message as the observation\n            return f\"[ERROR] Error during execution of tool '{tool_name}': {e}\"\n\n    def _parse_tool_calls(self, content: str):\n        \"\"\"Try parse the tool calls.\"\"\"\n        matches = re.findall(r'<tool_call>(.*?)</tool_call>', content, re.DOTALL)\n        tool_calls = []\n        for match in matches:\n            try:\n                tool_call_data = json.loads(match.strip())\n                tool_calls.append(tool_call_data)\n            except json.JSONDecodeError as e:\n                self.log_func(f\"[ERROR] JSON decode failed on tool call: {e}\")\n        return tool_calls\n\n    def run(self, task: str) -> Any:\n        \"\"\"\n        Runs the agent loop starting with the initial messages.\n\n        Args:\n            task: The initial user prompt.\n\n        Returns:\n            The final answer extracted from the final_answer tool call,\n            or None if max_steps is reached or an error occurs.\n        \"\"\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": task}\n        ]\n\n        self.log_func(f\"[INFO] Starting agent run with max_steps={self.max_steps}\")\n        # self.log_func(\"[INFO] Initial messages:\")\n        # self.log_func(json.dumps(messages, indent=2))\n        self.log_func(\"-\" * 30)\n\n        for step in range(self.max_steps):\n            self.log_func(f\"----------- Step {step + 1} -----------\")\n            # self.log_func(json.dumps(messages, indent=2))\n\n            # For guidance on handling and executing function calls, please refer to [this documentation](https://docs.sglang.ai/backend/function_calling.html).\n            # self.log_func(f\"[INFO] Current message at step {step + 1}: {messages}\")\n            current_prompt = self.tokenizer.apply_chat_template(\n                messages,\n                tools=self.tools_definitions,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            # self.log_func(f\"[INFO] Current prompt at step {step + 1}: {current_prompt}\")\n            response = self.fix_closing_tags(model(current_prompt))\n            self.log_func(f\"[INFO] Model response at step {step + 1}: {response}\")\n            messages.append({\"role\": \"assistant\", \"content\": response})\n\n            tool_calls = self._parse_tool_calls(response)\n            if len(tool_calls) == 0:\n                self.log_func(\"[INFO] No tool calls this round.\")\n                self.log_func(\"-\" * 30)\n                continue\n\n            for tool_call in tool_calls:\n                try:\n                    tool_name = tool_call[\"name\"]\n                    parsed_args = tool_call.get(\"arguments\", {})\n\n                    result = self._execute_tool(tool_name, parsed_args)\n                    # self.log_func(f\"[INFO] Result from tools: {result}\")\n                    messages.append({\n                        \"role\": \"assistant\",\n                        \"content\": f\"Tool {tool_name} has obtained the results {result}\"\n                    })\n\n                    if tool_name == self.final_answer_tool_name:\n                        self.log_func(f\"[INFO] Final answer received: {result}\")\n                        return result\n                except Exception as e:\n                    self.log_func(f\"[ERROR] Failed to execute tool call: {e}\")\n            \n            self.log_func(\"-\" * 30) # Separator for next step\n\n        self.log_func(\"[INFO] Maximum steps reached without a final answer.\")\n        return None","metadata":{"id":"hb5YdJJHSfSU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\ndef run_simpleqa_tool_calling_agent(task=\"simpleqa\", model_id=model_id, action_type=\"tool_calling\"):\n    search_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n    search_agent = ToolCallAgent(\n        model=model,\n        tools=[GoogleSearchTool(), VisitWebpageTool(), FinalAnswerTool()],\n        max_steps=10\n    )\n    answer_questions(task, search_agent, action_type, search_agent_answers_file)\n    df = score_answers([search_agent_answers_file])\n    print(df)\n\nrun_simpleqa_tool_calling_agent()","metadata":{"id":"CwKOxzveSCDD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget -q -O local_python_executor.py https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/local_python_executor.py","metadata":{"id":"oco5MCGVOJt_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom local_python_executor import (\n    BASE_BUILTIN_MODULES,\n    BASE_PYTHON_TOOLS,\n    evaluate_python_code,\n)\n\n\nclass PythonInterpreterTool(Tool):\n    name = \"python_interpreter\"\n    description = \"This is a tool for evaluating python code. It can be used to perform calculations. To generate valid Python code, first draft your code in Markdown using ```python```, then write it as a one-line string in the `code` field. Make sure it is valid JSON using double quotes and escaped newlines.\"\n    inputs = {\n        \"code\": {\n            \"type\": \"string\",\n            \"description\": \"The python code to run in interpreter\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, *args, authorized_imports=None, **kwargs):\n        if authorized_imports is None:\n            self.authorized_imports = list(set(BASE_BUILTIN_MODULES))\n        else:\n            self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(authorized_imports))\n        self.inputs = {\n            \"code\": {\n                \"type\": \"string\",\n                \"description\": (\n                    \"The code snippet to evaluate. All variables used in this snippet must be defined in this same snippet, \"\n                    f\"else you will get an error. This code can only import the following python libraries: {self.authorized_imports}.\"\n                ),\n            }\n        }\n        self.base_python_tools = BASE_PYTHON_TOOLS\n        self.python_evaluator = evaluate_python_code\n        super().__init__(*args, **kwargs)\n\n    def forward(self, code: str) -> str:\n        state = {}\n        output = str(\n            self.python_evaluator(\n                code,\n                state=state,\n                static_tools=self.base_python_tools,\n                authorized_imports=self.authorized_imports,\n            )  # The second element is boolean is_final_answer\n        )\n        return f\"Stdout:\\n{str(state['_print_outputs'])}\\nOutput: {output}\"","metadata":{"id":"vBUuNgOIWoA7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom eval_utils import score_answers\n\ndef run_math_tool_calling_agent(task=\"math\", model_id=model_id, action_type=\"tool_calling\"):\n    math_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n    math_agent = ToolCallAgent(\n        model=model,\n        tools=[PythonInterpreterTool(authorized_imports=[\"numpy\", \"sympy\"]), FinalAnswerTool()],\n        max_steps=10\n    )\n    answer_questions(task, math_agent, action_type, math_agent_answers_file)\n    df = score_answers([math_agent_answers_file])\n    print(df)\n\nrun_math_tool_calling_agent()","metadata":{"id":"Ofg3MzBzWZ0x","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\n#  The following helper code is given to you.\n######################################################\n\nfrom eval_utils import score_answers\n\ndef run_gaia_tool_calling_agent(task=\"gaia\", model_id=model_id, action_type=\"tool_calling\"):\n    research_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n    research_agent = ToolCallAgent(\n        model=model,\n        tools=[GoogleSearchTool(), VisitWebpageTool(), PythonInterpreterTool(authorized_imports=[\"numpy\", \"sympy\"]), FinalAnswerTool()],\n        max_steps=10\n    )\n    answer_questions(task, research_agent, action_type, research_agent_answers_file)\n    df = score_answers([research_agent_answers_file])\n    print(df)\n\nrun_gaia_tool_calling_agent()","metadata":{"id":"Rkg8Br0icJMB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"| Task        | Action Type | Performance      |\n|-------------|-----|-----------------|\n| SimpleQA       | vanilla  |  |\n| SimpleQA       | tool calling  |  |\n| MATH       | vanilla  |  |\n| MATH       | tool calling  |  |\n| GAIA       | vanilla  |  |\n| GAIA       | tool calling  |  |","metadata":{"id":"6xWWEdTId0FU"}}]}